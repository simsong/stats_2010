# Configuration file for database reconstruction
# Environment variables in this file will be expanded first.
#
# If you need different variables for different systems, you add the system after an @ sign.
# For example, to set HOME to be /usr/home/system on every system except 'vacant', where the HOME
# is set to be /usr/home/vacant, use:
#  [section]
#  HOME=/usr/home/system
#  HOME@vacant=/usr/home/vacant
#
#

[run]
# Time between system
sleep_time=10

# Name of this system:
name=recon

# Max number of threads to use (unless changed)
threads=1

# Maximum number of LP files to make at once
lp_j2 = 4

# Make sure that we leave cbsuecco02.biohpc.cornell.edu mostly for running the solver
# This should be modified so that the largest LP files are made on the large memory machine.
# Unfortunately, right now we have no way to figure out which they re
max_lp = 2
max_lp@cbsuecco02.biohpc.cornell.edu = 4
max_lp@cbsuecco03.biohpc.cornell.edu = 2
max_lp@cbsuecco05.biohpc.cornell.edu = 2
max_lp@cbsuecco07.biohpc.cornell.edu = 1
max_lp@cbsuecco08.biohpc.cornell.edu = 1

# Maximum number of Gurobi instances to run at once
max_sol = 0
max_sol@cbsuecco02.biohpc.cornell.edu = 16

# maximum number of solutions we can launch at once
max_sol_launch = 20

# Maximum load where we can still schedule
max_load = 32

# Maximum number of jobs that can run at once
max_jobs = 32
max_jos@cbsuecco02.biohpc.cornell.edu = 64


[urls]
SF1_URL_TEMPLATE=http://www2.census.gov/census_2010/04-Summary_File_1/{state_name}/{state_abbr}2010.sf1.zip

[paths]
# Where SF1 ZIP files distributed by the US Census Bureau are stored:
SF1_DIST=$HOME/2010_sf1_dist

# Where the output goes
ROOT=/home/ecco_lv39/projects/2010_recon
#ROOT=/home/slgarfinkel/2010_recon
#ROOT=/workdir/slgarfinkel/2010_recon

[gurobi]
threads: 8
customer: Census
customer@cbsuecco02.biohpc.cornell.edu:
appname: DAS


# Just grab the mysql values from the environment variables
[mysql]
host: $MYSQL_HOST
database: $MYSQL_DATABASE
user: $MYSQL_USER
password: $MYSQL_PASSWORD
