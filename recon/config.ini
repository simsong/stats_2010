# Configuration file for database reconstruction
# Environment variables in this file will be expanded first.
#
# If you need different variables for different systems, you add the system after an @ sign.
# For example, to set HOME to be /usr/home/system on every system except 'vacant', where the HOME
# is set to be /usr/home/vacant, use:
#  [section]
#  HOME=/usr/home/system
#  HOME@vacant=/usr/home/vacant
#
# This file is designed to be read with the version of the config file reader in the dbrecon.py
# module. A more sophisticated hiearchical config file reader is in ctools.
#
# This file has been updated so that the REIDENT environment variable
# specifies which reconstruction experiment identifier is in use.

[run]
# Time between system
sleep_time=10

# Name of this system:
name=recon

# Max number of threads to use (unless changed)
threads=1

# Maximum number of LP files to make at once
# There are two multithreading points.
# Remember that making LP files is memory-constrained, but that really only matters for CA and TX
# This should have selectiosn by state.
lp_j2 = 16

# Mak
max_lp = 2

# Prevously we worked at Cornell's BioHPC, which had different machines of different sizes.
# This construction allows different values of the config options to have different values
# when read on different machines

# max_lp@cbsuecco02.biohpc.cornell.edu = 4
# max_lp@cbsuecco03.biohpc.cornell.edu = 2
# max_lp@cbsuecco05.biohpc.cornell.edu = 2
# max_lp@cbsuecco07.biohpc.cornell.edu = 1
# max_lp@cbsuecco08.biohpc.cornell.edu = 1

# Maximum number of Gurobi instances to run at once on each system.
# 0 is unlimited
max_sol = 32
#max_sol@cbsuecco02.biohpc.cornell.edu = 32

# maximum number of solutions we can launch at once on each system
max_sol_launch = 20

# Maximum load where we can still schedule. If the load goes over this, new
# jobs are not created. This can be set to be the number of cores, although the load tends to go higher.
max_load = 96

# Maximum number of jobs that can run at once
max_jobs = 96
# max_jobs@cbsuecco02.biohpc.cornell.edu = 64


[urls]
# Where the downloader gets the ZIP files from
SF1_URL_TEMPLATE=http://www2.census.gov/census_2010/04-Summary_File_1/{state_name}/{state_abbr}2010.sf1.zip

[paths]
# Where SF1 ZIP files distributed by the US Census Bureau are stored:
SF1_DIST=$DAS_S3ROOT/2010-re/$REIDENT/dist

# On nimi we put it in a different place
SF1_DIST@nimi.local:$HOME/gits/stats_2010/sf1/

# Where the output goes
ROOT=$DAS_S3ROOT/2010-re/$REIDENT

# This was when I was running at home on a computer called nimi
ROOT@nimi.local:$HOME/2010_recon

[gurobi]
# Gurobi configuration
# We set threads to 32. We've learned that additional threads don't take a lot of memory
# and there is no problem raising the load to >1000 on the DAS runs when there are 96 cores.
threads: 32
customer: Census
appname: DAS
TOKENSERVER=mr7dassv001.ti.census.gov,mr7dassv002.ti.census.gov
PORT=41954


# Just grab the mysql values from the environment variables
[mysql]
host: $MYSQL_HOST
database: $MYSQL_DATABASE
user: $MYSQL_USER
password: $MYSQL_PASSWORD
# if verbose is 1, all MySQL queries are displayed.
verbose: 1
