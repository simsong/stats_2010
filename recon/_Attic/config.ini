# Configuration file for database reconstruction
# Environment variables in this file will be expanded first.
# In the Census environment, we have S3ROOT to be set to the AWS S3 directory where we store the files for this project
#
# If you need different variables for different systems, you add the system after an @ sign.
# For example, to set HOME to be /usr/home/system on every system except 'vacant', where the HOME
# is set to be /usr/home/vacant, use:
#  [section]
#  HOME=/usr/home/system
#  HOME@vacant=/usr/home/vacant
#

[run]
# Name of this system:
name=recon

# Number of threads to use:
threads=16

[urls]
SF1_URL_TEMPLATE=http://www2.census.gov/census_2010/04-Summary_File_1/{state_name}/{state_abbr}2010.sf1.zip


[paths]
CACHE_DIR=/tmp/geocache


# Where SF1 ZIP files distributed by the US Census Bureau are stored:
SF1_DIST=$S3ROOT/2010r2/sf1dist
SF1_DIST@ir7dassv001.ite.ti.census.gov=/data/sf1/dist
SF1_DIST@ir7dassv002.ite.ti.census.gov=/data/sf1/dist
SF1_DIST@ir7dassv003.ite.ti.census.gov=/data/sf1/dist
SF1_DIST@vdi-vdesk-4618=/cygdrive/h/sf1
SF1_DIST@vdi-vdesk-0509=/cygdrive/h/sf1

# Where the output goes
ROOT=/data/sf1/recon

# Where Step3 should read its SNCOUNT files from
SF1DATA_ROOT=$S3ROOT/title13_reid_cleanup/solution_variability

# Where Step3 should write its LP files to
#LPROOT=$S3ROOT/title13_reid_cleanup/simson_runs
LPROOT=/data/sf1/recon-badblocks
#LPROOT=/data/garfi303/das-reid-processing/02_reconstruction/primary/tammy

# These are for testing the reconstruction system
GOLD_ROOT=/data/garfi303/das-reid-processing/02_reconstruction/primary/gold
TEST_ROOT=/data/garfi303/das-reid-processing/02_reconstruction/primary/test


# Backup tammy files:
# aws s3 ls s3://uscb-decennial-ite-das/title13_reid_cleanup/tammy_recon_backup_s3/tammy_backup_targzs/
